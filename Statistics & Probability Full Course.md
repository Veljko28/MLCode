[Video Link](https://www.youtube.com/watch?v=sbbYntt5CJk)

# Module 1

## 1.1 Introduction

**Statistics is the science of gathering, describing, and analyzing data.**

A target population is a particular group of interest.
 - A sampled population is a group from which the sample is taken
 - A sampling frame is a physical list of all members of the sampled population.
 - A sample is a subset of the population from which data are collected.
 
A census is a study in which data are obtained from every member of the population.


A variable is a value of characteristic that changes among members of the population.

Data are the counts, measurements, or observations gathered about a specific variable in a population in order to study it.

A parameter is a numerical description of a population characteristic.

A sample statistic is a (numerical) description of a sample characteristic.

| Population | Sample | 
| -------- | -------- | 
| **Whole** Group | **Part** of the group |
| Group we **want to know** about | Group we **do know** about | 
| Characteristics are called **parameters**| Characteristics are called **statistics** | 
| Parameters are generally **unknown** | Statistics are always **known** | 
| Parameters are **fixed** | Statistics **change** with the sample | 


The branch of **descriptive statistics**, as a science, gathers, sorts, summarizes, and displays the data.

The branch of **inferential statistics**, as a science, involves using descriptive statistics to estimate population parameters.


**Exploratory analysis** uses data to estimate parameters.

**Confirmatory analysis** uses statistics to test claims about reality (hypotheses).


## 1.2 Data Classification

**Qualitative data**, also known as categorical data, consist of labels or descriptions of traits.

**Quantitative data**, also known as numeric data, consist of counts or measurements.


**Discrete data** are quantitative data that can take on only particular values and are usually counts.

**Continuous data** are quantitative data that can take on any value in a given interval and are usually measurements.


The 4 levels of measurement:
- **Nominal** -- description
- **Ordinal** -- ordering
- **Interval** -- differences between levels
- **Ratio** -- true zero

Data at the **nominal level** of measurement are qualitative data consisting of labels or names.

Data at the **ordinal level** of measurement are qualitative data that can be arranged in a meaningful order, but calculations such as addition or division do not make sense.

Data at the **interval level** of measurement are quantitative data that can be arranged in a meaningful order, and differences between data entries are meaningful.

Data at the **ratio level** of measurement are quantitative data that can be ordered, differences between data entries are meaningful, and the zero point indicates the absence of something.


## 1.3 The Process of a Statistical Study

#### Conducting a Statistical Study

1. Determine the design of the study.
   a. State the question to be studied.
   b. Determine the population and variables.
   c. Determine the sampling method.
2. Collect the data
3. Organize the data
4. Analyze the data to answer the question

An **observational study** observes data that already exist.

An **experiment** generates data to help identify cause-and-effect relationships.

A **representative sample** has the same *relevant* characteristics as the population and does not favor one group from the population over another.

---
**Simple Random Sample** - Equal chance of being selected 

**Stratified Sample** - Divide into subgroups called strata with things in common. Get a measurement from each strata

**Cluster Sample** - Divide into clusters that don't particularly have things in common.  Get a measurement from each cluster

**Systematic Sample** - Select nth of the population for the sample

---
In a **cross-sectional study**, data are collected at a single point in time.

In a **longitudinal study**, data are gathered by following a particular group over a period of time.

---

A **treatment** is some condition that is applied to a group of subjects in an experiment

The **subject (participants)** are people or things being studied in an experiment.

The **response (dependent) variable** is the variable in an experiment that responds to the treatment.

The **explanatory (independent) variable** is the variable in an experiment that causes the change in the response variable.

---

#### Principles of Experimental Design

1. Randomize the control and treatment groups
2. Control for outside effects on the response variable
3. Replicate the experiment a significant number of times to see meaningful patterns


A **control group** is a group to which no treatment is given.

A **treatment group** is a group which is given the treatment.


The **confounding variables** are *unmeasured* factors other than the treatment that cause an effect on the subjects of an experiment.

---

A **placebo** is a substance that appears identical to the actual treatment but contains no intrinsic beneficial elements.

The **placebo effect** is a response to the power of suggestion, rather than the treatment itself, by participants of an experiment. 

In a single-blind experiment the subject doesn't know he is in an experiment.

In a double-blind experiment both the subject and the measurer don't know that they are in an experiment.

An **Institutional Review Board (IRB)** is a group of people who review the design of a study to make sure that it is appropriate and that no unnecessary harm will come to the subjects involved.

# Module 2 - Summarizing using graphs 


## 2.1 Frequency Distributions 

A **distribution** is a way to describe the structure of a particular data set or population. 

A **frequency distribution** is a display of the values that occur in a data set and how often each value, or range of values, occurs.

**Frequencies** are the numbers of data values in the categories of a frequency distribution.

A **class** is a category of data in a frequency distribution.


## 2.2 Graphical Displays of Data


### Graphs
 - Should be able to stand alone without the original data.
 - Must have labels and values for both axes.
 - When appropriate, a legend, a source, and a date should be included.
 - In a paper, the graphic must contain a number and a caption. The graphic must also be discussed in the prose (writing).

Avoid pie charts, use bar charts instead.

A line graph is used when data are measurements over time. The horizontal axis represents time. The vertical axis represents the variable being measured. Straight lines are used to connect points plotted at the value of each measurement above the time it was taken.


## 2.3 Analyzing Graphs

Don't use pictographs (graphs with pictures instead of bars).

### Shapes of Graphs
1. Uniform
2. Symmetric
3. Skewed to the Right
4. Skewed to the Left

An outlier is a data value that falls outside the normal shape of the graph.

# Module 3 - Summarizing using numbers

## 3.1 Measures of Center

Getting a measure of center is getting the value that represents all the values from the dataset in a single point. It is usually the mean, median or the mode of the data set.

The sample mean has a nice distribution (Central Limit Theorem)

The weighted mean is the mean of a data set in which each data value in the set does not hold the same relative importance.

The mode of a data set is the most-observed value in the given data set.

The mode is a value in the data set that occurs most frequently.
- Unimodal: One value occurs most often.
- Bimodal: Exactly two values occur equally often.
- Multimodal: More than two values occur equally often.
- No model: Everything occurs the same number of times


### The Hildebrand Ratio

Finding the ratio is defined as the difference between the sample mean and the sample median divided by the standard deviation.

If the absolute value of the Hildebrand ratio is less than 0.2 that the data is sufficiently symmetric.


## 3.2 Measures of Dispersion

Range = Maximum Data Value - Minimum Data Value

The **population variance** is the average squared distance of the population values from the mean.

### Standard deviation

The **standard deviation** is a measure of how much we might expect a typical member of the data set to differ from the mean. Variance and standard deviation formulas are different.

### Coefficient of Variation

The **Coefficient of Variation** for a set of data is the ratio of the standard deviation to the mean as a percentage. For a population, it is given by:

CV = s / (x bar) \* 100%

s - standard deviation
x bar - mean

### Empirical Rule

When the data follow a bell-shaped distribution, an interesting pattern emerges in the data values:
- Approximately 68% of the data values lie within one standard deviation of the mean;
- Approximately 95% of the data values lie within two standard deviation of the mean;
- Approximately 99.7% of the data values lie within three standard deviation of the mean


### Chebyshev's Theorem (Чебышев)

The proportion of data that lie within K standard deviations of the mean is at least 1 - 1/K^2; for K > 1

so if K = 2, at least 3/4 of the data fall in the second deviation
if K = 3 than at least 8/9 of the data and so on

It gives the lower bound on the percentage of data that fall in the deviation


## 3.3 Measures of Relative Position

Percentile can be calculated as: 
**l = n \* P/100**

Where: 
n - number of values in the data set
P - the percentile you are looking for
l - distance from the first (so if you get l = 4, than the fourth value of your data set in the P-th percentile)

if you get l as a decimal number, round it up to the next, if you get a whole value calculate the mean of the l-th and the (l+1)-th number in the data set.

Box plot (box-and-whisker) can represent quartal data. Where:
- The first line represents the minimum
- The first side of the box represents Q1
- The second side of the box represents Q2
- The middle of the box is the median
- The last line represents the Maximum

We can compare two data values from different populations by comparing their respective percentiles. We could also determine how the values relate to the respective means of their data sets. This measure is called the **standard score**, or **z-score**.


The standard score fore a sample value is given by
z = x - (x bar) / s

x - value of interest
x bar - sample mean
s - sample standard deviation


# Module 4

## 4.1 Introduction to Probability

**Subjective (Bayesian) probability**, is simply an educated guess regarding the chance that an event will occur. It is the least precise type of probability.

In **experimental probability**, if E is an event, then P(E), read "the probability that E occurs", is given by

P(E) = f/n

where:
f - the frequency of event E
n - number of time the experiment is performed

In **classical probability**, if all outcomes are equally likely to occur, then P(E), read "the probability that E occurs," is given by 

P(E) = n(E) / n(S)

n(E) - numbers of elements in the event
n(S) - number of elements in the sample space


## 4.2 Addition Rules of Probability 

The **complement** of an event E, denoted Ec, is the set of all outcomes in the sample space that are not in E.

**P(E) + P(Ec) = 1**

If the events are mutually exclusive: 
	**P = P(E) + P(T)**
otherwise:
	**P = P(E) + P(T) - P(E + T)**


## 4.3 Multiplication Rules for Probability

Multiplication is used in multistage experiments.

For independent events: **P(E and F) = P(E) \* P(F)**

Two events are dependent if one event happening affects the probability of the other event happening.

Conditional probability, denoted P(F|E) and read "the probability of F given E," is the probability of event F occurring given that event E occurs first.

For two dependent events, E and F, the probability that E and F occur is given by the following formula.

**P(E and F) = P(E) \* P(F|E) \* P(F) \* P(E|F)**

For two dependent events, E and F, the probability that F occurs given that E occurs is given by the following formula.

**P(F|E) = P(E and F) / P(E)**

Order matters with conditional probability. P(F|E) ≠ P(E|F) in most cases

## 4.4 Combinations and Permutations

Factorial = n!

A **combination** is a selection of objects from a group without regard to their arrangement.

A **permutation** is a selection of objects from a group where the arrangement is specific.

## 4.5 Combining Probability and Counting Techniques

Just a lot of examples of probability and counting together.


# Module 5

## 5.1 Discrete Distributions

A **random variable** is a variable whose numeric value is determined by the outcome of a probability experiment.

Random variables follow a probability distribution.

The **expected value** (or mean) of a discrete random variable X is equal to the mean of the probability distribution of X and is given by 

**E\[X] = Σ x\*P\[X == x]**

The variance of a distribution (or of a random variable) is a measure of uncertainty in each outcome. It has the opposite meaning of precision.

The **variance** of a discrete random variable X is given by
**V\[X] = Σ(x-µ)^2\* P\[X == x]**


## 5.2 The Binomial Distribution

A random variable with only two possible outcomes follows a **Bernoulli distribution** with success probability parameter, p.

P\[X == x] = p^x \* (1-p)^(k1-x)

The cumulative distribution function (CDF) is the function defined as F(x) = P\[X <= x]

A Binomial random variable is the sum of n independent and identically distributed Bernoulli random variables.

A Binomial random variable meets the following five requirements:
- The number of trials, n, is known;
- each trial has two possible outcomes (is Bernoulli)
-  the success probability, p, is constant from trial to trial
- the trials are independent
- the random variable is the number of successes

## 5.3 The Poisson Distribution

A random variable that is the count of successes over an area or a time period follows a Poisson distribution with average rate parameter, λ (lambda).

Both the mean (estimated value) and the variance of a Poisson Distribution are λ.

## 5.4 The Hypergeometric 

The **Hypergeometric distribution** describes the probability of obtaining x success in k draws (trials), *without replacement*, from a finite population that contains exactly m successes and n failures.   

The expected value in general is equal to E\[X] = sample size x successes/ trials


# Module 6

## 6.0a The Uniform Distribution

A **continuous random variable** is a random variable with a sample space consisting of an interval of values.

Examples of continuous random variables:
- Student height
- Age of car
- Time spent at a stop light
- Distance a golf ball goes

The **Uniform distribution** is a continuous distribution that describes random variables whose likelihood of occurring is constant across a specified interval.


## 6.0b The Exponential Distribution  
  
The uniform distribution has a definite upper bound and a definite lower bound but we don't know what the variables is. In the exponential distribution we know the lower bound to be 0 and no defined upper bound.  
  
The **Exponential distribution** is a continuous distribution that describes random variables whose probability of occurring decreases with time. It is frequently used to mode "time until" something happens when there is no upper bound.  
  
- One parameter: λ, the rate of the thing happening  
- Sample space S = (0, ∞)  
- E\[X] = 1/λ  
- V\[X] = 1/λ^2  
  
Probabilities are areas under the density curve.  
  
  
## 6.1-4 The Normal Distribution  
  
The Normal Distribution is also called the Gaussian distribution.  
  
The parameter **μ** of the distribution (and also its median and mode), while the parameter **σ** is its standard deviation. The variance of the distribution is **σ²**.  
  
## 6.5 Approximating the Binomial  
  
  
- The approximation is better for larger values of n  
- The approximation is better for values of p close to 0.500  
  
Combining these two observations leads to the following "rule of thumb":  
- The Normal distribution is "sufficiently" close to the Binomial distribution if both np >= 5 (10 or 15 depending on the problem) and n(1-p) >= 5 (10 or 15)  
- The approximation is improved by using a "continuity correction" of 0.50 (added to <=, subtracted from >=).  
  
A **continuity correction factor** is used when you use a continuous probability distribution to approximate a discrete probability distribution.  
  
  
# Module 7  
  
## 7.1 The Central Limit Theorem  
  
  
Let X be a random variable with mean  **μ** and a finite variance **σ²**. Let us draw a random sample of size n from this distribution.  
  
Then, the distribution of the sample sums converges to a Normal distribution as n gets larger. Specifically,  
  
Σ Xi -> d N(nμ, nσ²)  
  
Where:  
Sum is over n.  
N is the normal distribution  
  
The Central Limit Theorem (CLT) tells us the following:  
- The sum of independent random variables is more Normal than the distribution of the variable itself, unless the variable is Normally distributed or if it has an infinite variance.  
- The Binomial is a sum of independent Bernoulli rvs (random variables)  
- The Poisson is a sum of independent Poisson rvs  
- Because the sample mean is just the sample sum, divided by a constant (n), the CLT tells us that the distribution of sample means will tend towards Normal.  
- The speed of convergence depends on how closely the data distribution is to Normal. The closer, the faster.  
  
## 7.2 The Distribution of Sample Means  
  
Bootstrapping is using the data under repeated sampling to estimate a population parameter. It's greatest use is to estimate the confidence interval for the mean.  
  
## 7.3 The Distribution of Sample Proportions  
  
Power is the ability to distinguish between a true and a false Normal hypothesis.  
  
As the sample size increases the precision increases as well.  
  
  
# Module 8  
  
## 8.\* Confidence intervals  
  
A **confidence interval** is a set of values that theoretically contain the population parameter a given proportion of the time when the experiment is performed many times.  
  
The confidence interval  
- gives information about the population parameter  
- is a a set of reasonable values from that parameter  
- is a function of the data (and thus a random variable)  
- is a result of a probability distribution calculation  
  
  
# Module 10  
  
## 10.\* The Theory of Hypothesis Testing  
  
The theory behind hypothesis testing is  
- State the research hypothesis and the null hypothesis  
- Determine how much the data support the hypothesis  
- Determine the parameter tested  
- Determine appropriate statistic  
- Determine distribution of that statistic under the null  
- Determine how likely it is to observe the statistic (data) if the null hypothesis is true  
- Interpret that level of support  
  
A **hypothesis** is a testable claim about reality.  
  
The "generic" parameter is *θ*.  
  
  
The **p-value** is the probability of observing a test statistic this extreme, or more so, given the null hypothesis is true.  
  
The **alpha value** is the Type I error rate claimed by the statistician.  
  
A **Type I error** occurs when the researcher rejects a true null hypothesis.  
  
A **Type II error** occurs when the researcher fails to reject a false null hypothesis. Its value is symbolized by **β**  
  
## 10.4 Handling Proportions


Since we are trying to draw conclusions about a single population proportion, we should use a test statistic based on the sample proportion (Wald test) or upon what we observe (Binomial test).


## 10.6 Discrete Distribution Matching

Chi-Square Goodness-of-Fit Procedure
- Null hypothesis: Data are generated by the hypothesized distribution
- Graphic: Binomial plot
- Requires: Expected number of successes is at least 5 in each group

It can be shown that this test statistic approximately follows a Chi-square distribution with k-1 degrees of freedom.

## 10.7 Categorical Independence

If two categorical variables are independent, then **the value of one variable does not change the probability distribution of the other**. If two categorical variables are related, then the distribution of one depends on the level the other.

The Chi-square test of independence _checks whether two variables are likely_ to be related or not. We have counts for two categorical or nominal variables.


# Module 11

## 11.6 Analysis of Variance (ANOVA)

The idea behind the Analysis of Variance procedure:
- Measure the variance of the original data (corrected)
- Measure the variance unexplained in the model
- Calculate the ratio of the explained variance to the unexplained 